{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DeepHit_Experiments.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"cells":[{"cell_type":"markdown","metadata":{"id":"C8XqUVdQtPZi"},"source":["# Deep learning-based time-to-event analysis for credit scoring\n","Gabriel Blumenstock"]},{"cell_type":"markdown","metadata":{"id":"-FZB8KV1tncO"},"source":["This Jupiter Notebook can be regarded as an online companion for the following article on deep learning for time-to-event analysis for credit scoring published in the Journal of the Operational Research Society:\n","\n","https://doi.org/10.1080/01605682.2020.1838960\n","\n","With this Notebook, the reader can re-implement all experiments that were performed with the deep learning-based model \"DeepHit\" within the scope of this publication using Google Colab. Please note that only the \"DeepHit\"-experiments are provided here. All statistical and random forest benchmark experiments were performed in R and will not be published here.\n","\n","The Notebook consists of four parts:\n","- In **Part 1**, packages are loaded and all necessary functions are defined.\n","- Hyperparameter tuning is implemented in **Part 2**.\n","- In **Part 3**, all experiments on performance evaluation are implemented.\n","- Finally, variable importance experiments can be found in **Part 4**.\n"]},{"cell_type":"markdown","metadata":{"id":"e5G83CTAtnQJ"},"source":["### Part 1: Preparations"]},{"cell_type":"markdown","metadata":{"id":"izRRVIbfutMp"},"source":["In this part, packages are imported and all necessary functions are defined. Thereby, most functions are based on the codes provided by Lee et at. (2018) via https://github.com/chl8856/DeepHit. Yet, some functions had to be modified and additional functions were defined to adapt to the specific setting of interest."]},{"cell_type":"code","metadata":{"id":"te8xayxMDpiV"},"source":["#import packages\n","!pip install tensorflow==1.12.0\n","import numpy as np\n","import os\n","import pandas as pd\n","import random\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","from tensorflow.contrib.layers import fully_connected as FC_Net\n","from termcolor import colored\n","import time, datetime, os\n","from google.colab import drive\n","from google.colab import files\n","drive.mount('/content/drive')\n","seed = 1234"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M9nEPZ16OLtZ"},"source":["#functions from github.com/chl8856/DeepHit\n","\n","\n","#UTILS_NETWORK\n","\n","### FEEDFORWARD NETWORK\n","def create_FCNet(inputs, num_layers, h_dim, h_fn, o_dim, o_fn, w_init, keep_prob=1.0, w_reg=None):\n","    # default active functions (hidden: relu, out: None)\n","    if h_fn is None:\n","        h_fn = tf.nn.relu\n","    if o_fn is None:\n","        o_fn = None\n","\n","    # default initialization functions (weight: Xavier, bias: None)\n","    if w_init is None:\n","        w_init = tf.contrib.layers.xavier_initializer() # Xavier initialization\n","\n","    for layer in range(num_layers):\n","        if num_layers == 1:\n","            out = FC_Net(inputs, o_dim, activation_fn=o_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n","        else:\n","            if layer == 0:\n","                h = FC_Net(inputs, h_dim, activation_fn=h_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n","                if not keep_prob is None:\n","                    h = tf.nn.dropout(h, keep_prob=keep_prob)\n","\n","            elif layer > 0 and layer != (num_layers-1): # layer > 0:\n","                h = FC_Net(h, h_dim, activation_fn=h_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n","                if not keep_prob is None:\n","                    h = tf.nn.dropout(h, keep_prob=keep_prob)\n","\n","            else: # layer == num_layers-1 (the last layer)\n","                out = FC_Net(h, o_dim, activation_fn=o_fn, weights_initializer=w_init, weights_regularizer=w_reg)\n","\n","    return out\n","\n","\n","#UTILS_EVAL\n","\n","### CONCORDANCE INDEX\n","def c_index(Prediction, Time_survival, Death, Time):\n","    N = len(Prediction)\n","    A = np.zeros((N,N))\n","    Q = np.zeros((N,N))\n","    N_t = np.zeros((N,N))\n","    Num = 0\n","    Den = 0\n","    for i in range(N):\n","        A[i, np.where(Time_survival[i] < Time_survival)] = 1\n","        Q[i, np.where(Prediction[i] > Prediction)] = 1\n","  \n","        if (Time_survival[i]<=Time and Death[i]==1):\n","            N_t[i,:] = 1\n","\n","    Num  = np.sum(((A)*N_t)*Q)\n","    Den  = np.sum((A)*N_t)\n","\n","    if Num == 0 and Den == 0:\n","        result = -1 # not able to compute c-index!\n","    else:\n","        result = float(Num/Den)\n","\n","    return result\n","\n","\n","#CLASS_DEEPHIT\n","\n","def log(x):\n","    return tf.log(x + 1e-8)\n","\n","def div(x, y):\n","    return tf.div(x, (y + 1e-8))\n","\n","### DEEPHIT NETWORK\n","class Model_DeepHit:\n","    def __init__(self, sess, name, input_dims, network_settings):\n","        self.sess               = sess\n","        self.name               = name\n","\n","        # INPUT DIMENSIONS\n","        self.x_dim              = input_dims['x_dim']\n","\n","        self.num_Event          = input_dims['num_Event']\n","        self.num_Category       = input_dims['num_Category']\n","\n","        # NETWORK HYPER-PARMETERS\n","        self.h_dim_shared       = network_settings['h_dim_shared']\n","        self.h_dim_CS           = network_settings['h_dim_CS']\n","        self.num_layers_shared  = network_settings['num_layers_shared']\n","        self.num_layers_CS      = network_settings['num_layers_CS']\n","\n","        self.active_fn          = network_settings['active_fn']\n","        self.initial_W          = network_settings['initial_W']\n","        self.reg_W              = tf.contrib.layers.l2_regularizer(scale=1.0)\n","        self.reg_W_out          = tf.contrib.layers.l1_regularizer(scale=1.0)\n","\n","        self._build_net()\n","\n","    def _build_net(self):\n","        with tf.variable_scope(self.name):\n","            #### PLACEHOLDER DECLARATION\n","            self.mb_size     = tf.placeholder(tf.int32, [], name='batch_size')\n","            self.lr_rate     = tf.placeholder(tf.float32, [], name='learning_rate')\n","            self.keep_prob   = tf.placeholder(tf.float32, [], name='keep_probability')   #keeping rate\n","            self.a           = tf.placeholder(tf.float32, [], name='alpha')\n","            self.b           = tf.placeholder(tf.float32, [], name='beta')\n","            self.c           = tf.placeholder(tf.float32, [], name='gamma')\n","\n","            self.x           = tf.placeholder(tf.float32, shape=[None, self.x_dim], name='inputs')\n","            self.k           = tf.placeholder(tf.float32, shape=[None, 1], name='labels')     #event/censoring label (censoring:0)\n","            self.t           = tf.placeholder(tf.float32, shape=[None, 1], name='timetoevents')\n","\n","            self.fc_mask1    = tf.placeholder(tf.float32, shape=[None, self.num_Event, self.num_Category], name='mask1')  #for Loss 1\n","            self.fc_mask2    = tf.placeholder(tf.float32, shape=[None, self.num_Category], name='mask2')  #for Loss 2 / Loss 3\n","\n","            ##### SHARED SUBNETWORK w/ FCNETS\n","            shared_out = create_FCNet(self.x, self.num_layers_shared, self.h_dim_shared, self.active_fn, self.h_dim_shared, self.active_fn, self.initial_W, self.keep_prob, self.reg_W)\n","            last_x = self.x  #for residual connection\n","\n","            h = tf.concat([last_x, shared_out], axis=1)\n","\n","            #(num_layers_CS) layers for cause-specific (num_Event subNets)\n","            out = []\n","            for _ in range(self.num_Event):\n","                cs_out = create_FCNet(h, (self.num_layers_CS), self.h_dim_CS, self.active_fn, self.h_dim_CS, self.active_fn, self.initial_W, self.keep_prob, self.reg_W)\n","                out.append(cs_out)\n","            out = tf.stack(out, axis=1) # stack referenced on subject\n","            out = tf.reshape(out, [-1, self.num_Event*self.h_dim_CS])\n","            out = tf.nn.dropout(out, keep_prob=self.keep_prob)\n","\n","            out = FC_Net(out, self.num_Event * self.num_Category, activation_fn=tf.nn.softmax, \n","                         weights_initializer=self.initial_W, weights_regularizer=self.reg_W_out, scope=\"Output\")\n","            self.out = tf.reshape(out, [-1, self.num_Event, self.num_Category])\n","\n","            ##### GET LOSS FUNCTIONS\n","            self.loss_Log_Likelihood()      #get loss1: Log-Likelihood loss\n","            self.loss_Ranking()             #get loss2: Ranking loss\n","            self.loss_Calibration()         #get loss3: Calibration loss\n","\n","            self.LOSS_TOTAL = self.a*self.LOSS_1 + self.b*self.LOSS_2 + self.c*self.LOSS_3\n","            self.solver = tf.train.AdamOptimizer(learning_rate=self.lr_rate).minimize(self.LOSS_TOTAL)\n","\n","    ### LOSS-FUNCTION 1 -- Log-likelihood loss\n","    def loss_Log_Likelihood(self):\n","        I_1 = tf.sign(self.k)\n","\n","        #for uncenosred: log P(T=t,K=k|x)\n","        tmp1 = tf.reduce_sum(tf.reduce_sum(self.fc_mask1 * self.out, reduction_indices=2), reduction_indices=1, keep_dims=True)\n","        tmp1 = I_1 * log(tmp1)\n","\n","        #for censored: log \\sum P(T>t|x)\n","        tmp2 = tf.reduce_sum(tf.reduce_sum(self.fc_mask1 * self.out, reduction_indices=2), reduction_indices=1, keep_dims=True)\n","        tmp2 = (1. - I_1) * log(tmp2)\n","\n","        self.LOSS_1 = - tf.reduce_mean(tmp1 + 1.0*tmp2)\n","\n","    ### LOSS-FUNCTION 2 -- Ranking loss\n","    def loss_Ranking(self):\n","        sigma1 = tf.constant(0.1, dtype=tf.float32)\n","\n","        eta = []\n","        for e in range(self.num_Event):\n","            one_vector = tf.ones_like(self.t, dtype=tf.float32)\n","            I_2 = tf.cast(tf.equal(self.k, e+1), dtype = tf.float32) #indicator for event\n","            I_2 = tf.diag(tf.squeeze(I_2))\n","            tmp_e = tf.reshape(tf.slice(self.out, [0, e, 0], [-1, 1, -1]), [-1, self.num_Category]) #event specific joint prob.\n","\n","            R = tf.matmul(tmp_e, tf.transpose(self.fc_mask2)) #no need to divide by each individual dominator\n","            # r_{ij} = risk of i-th pat based on j-th time-condition (last meas. time ~ event time) , i.e. r_i(T_{j})\n","\n","            diag_R = tf.reshape(tf.diag_part(R), [-1, 1])\n","            R = tf.matmul(one_vector, tf.transpose(diag_R)) - R # R_{ij} = r_{j}(T_{j}) - r_{i}(T_{j})\n","            R = tf.transpose(R)                                 # Now, R_{ij} (i-th row j-th column) = r_{i}(T_{i}) - r_{j}(T_{i})\n","\n","            T = tf.nn.relu(tf.sign(tf.matmul(one_vector, tf.transpose(self.t)) - tf.matmul(self.t, tf.transpose(one_vector))))\n","            # T_{ij}=1 if t_i < t_j  and T_{ij}=0 if t_i >= t_j\n","\n","            T = tf.matmul(I_2, T) # only remains T_{ij}=1 when event occured for subject i\n","\n","            tmp_eta = tf.reduce_mean(T * tf.exp(-R/sigma1), reduction_indices=1, keep_dims=True)\n","\n","            eta.append(tmp_eta)\n","        eta = tf.stack(eta, axis=1) #stack referenced on subjects\n","        eta = tf.reduce_mean(tf.reshape(eta, [-1, self.num_Event]), reduction_indices=1, keep_dims=True)\n","\n","        self.LOSS_2 = tf.reduce_sum(eta) #sum over num_Events\n","\n","    ### LOSS-FUNCTION 3 -- Calibration Loss\n","    def loss_Calibration(self):\n","        eta = []\n","        for e in range(self.num_Event):\n","            one_vector = tf.ones_like(self.t, dtype=tf.float32)\n","            I_2 = tf.cast(tf.equal(self.k, e+1), dtype = tf.float32) #indicator for event\n","            tmp_e = tf.reshape(tf.slice(self.out, [0, e, 0], [-1, 1, -1]), [-1, self.num_Category]) #event specific joint prob.\n","\n","            r = tf.reduce_sum(tmp_e * self.fc_mask2, axis=0) #no need to divide by each individual dominator\n","            tmp_eta = tf.reduce_mean((r - I_2)**2, reduction_indices=1, keep_dims=True)\n","\n","            eta.append(tmp_eta)\n","        eta = tf.stack(eta, axis=1) #stack referenced on subjects\n","        eta = tf.reduce_mean(tf.reshape(eta, [-1, self.num_Event]), reduction_indices=1, keep_dims=True)\n","\n","        self.LOSS_3 = tf.reduce_sum(eta) #sum over num_Events\n","    \n","    def get_cost(self, DATA, MASK, PARAMETERS, keep_prob, lr_train):\n","        (x_mb, k_mb, t_mb) = DATA\n","        (m1_mb, m2_mb) = MASK\n","        (alpha, beta, gamma) = PARAMETERS\n","        return self.sess.run(self.LOSS_TOTAL, \n","                             feed_dict={self.x:x_mb, self.k:k_mb, self.t:t_mb, self.fc_mask1: m1_mb, self.fc_mask2:m2_mb, \n","                                        self.a:alpha, self.b:beta, self.c:gamma, \n","                                        self.mb_size: np.shape(x_mb)[0], self.keep_prob:keep_prob, self.lr_rate:lr_train})\n","\n","    def train(self, DATA, MASK, PARAMETERS, keep_prob, lr_train):\n","        (x_mb, k_mb, t_mb) = DATA\n","        (m1_mb, m2_mb) = MASK\n","        (alpha, beta, gamma) = PARAMETERS\n","        return self.sess.run([self.solver, self.LOSS_TOTAL], \n","                             feed_dict={self.x:x_mb, self.k:k_mb, self.t:t_mb, self.fc_mask1: m1_mb, self.fc_mask2:m2_mb, \n","                                        self.a:alpha, self.b:beta, self.c:gamma, \n","                                        self.mb_size: np.shape(x_mb)[0], self.keep_prob:keep_prob, self.lr_rate:lr_train})\n","\n","    def predict(self, x_test, keep_prob=1.0):\n","        return self.sess.run(self.out, feed_dict={self.x: x_test, self.mb_size: np.shape(x_test)[0], self.keep_prob: keep_prob})\n","\n","def f_get_minibatch(mb_size, x, label, time, mask1, mask2):\n","    idx = range(np.shape(x)[0])\n","    idx = random.sample(idx, mb_size)\n","\n","    x_mb = x[idx, :].astype(np.float32)\n","    k_mb = label[idx, :].astype(np.float32) # censoring(0)/event(1,2,..) label\n","    t_mb = time[idx, :].astype(np.float32)\n","    m1_mb = mask1[idx, :, :].astype(np.float32) #fc_mask\n","    m2_mb = mask2[idx, :].astype(np.float32) #fc_mask\n","    return x_mb, k_mb, t_mb, m1_mb, m2_mb\n","\n","def get_valid_performance(DATA, MASK, in_parser, eval_time):\n","    ##### DATA & MASK\n","    (data, time, label)  = DATA\n","    (mask1, mask2)       = MASK\n","\n","    x_dim                       = np.shape(data)[1]\n","    _, num_Event, num_Category  = np.shape(mask1)  # dim of mask1: [subj, Num_Event, Num_Category]\n","    \n","    ACTIVATION_FN               = {'relu': tf.nn.relu, 'elu': tf.nn.elu, 'tanh': tf.nn.tanh}\n","\n","    ##### HYPER-PARAMETERS\n","    mb_size                     = in_parser['mb_size']\n","\n","    iteration                   = in_parser['iteration']\n","\n","    keep_prob                   = in_parser['keep_prob']\n","    lr_train                    = in_parser['lr_train']\n","\n","\n","    alpha                       = in_parser['alpha']  #for log-likelihood loss\n","    beta                        = in_parser['beta']  #for ranking loss\n","    gamma                       = in_parser['gamma']  #for RNN-prediction loss\n","    parameter_name              = 'a' + str('%02.0f' %(10*alpha)) + 'b' + str('%02.0f' %(10*beta)) + 'c' + str('%02.0f' %(10*gamma))\n","\n","    initial_W                   = tf.contrib.layers.xavier_initializer()\n","\n","    ##### MAKE DICTIONARIES\n","    # INPUT DIMENSIONS\n","    input_dims                  = { 'x_dim'         : x_dim,\n","                                    'num_Event'     : num_Event,\n","                                    'num_Category'  : num_Category}\n","    # NETWORK HYPER-PARMETERS\n","    network_settings            = { 'h_dim_shared'       : in_parser['h_dim_shared'],\n","                                    'num_layers_shared'  : in_parser['num_layers_shared'],\n","                                    'h_dim_CS'           : in_parser['h_dim_CS'],\n","                                    'num_layers_CS'      : in_parser['num_layers_CS'],\n","                                    'active_fn'          : ACTIVATION_FN[in_parser['active_fn']],\n","                                    'initial_W'          : initial_W }\n","\n","    ##### CREATE DEEPHIT NETWORK\n","    tf.reset_default_graph()\n","\n","    config = tf.ConfigProto()\n","    config.gpu_options.allow_growth = True\n","    sess = tf.Session(config=config)\n","\n","    model = Model_DeepHit(sess, \"DeepHit\", input_dims, network_settings)\n","    saver = tf.train.Saver()\n","\n","    sess.run(tf.global_variables_initializer())\n","\n","    ### TRAINING-TESTING SPLIT\n","    (tr_data,va_data, tr_time,va_time, tr_label,va_label, \n","     tr_mask1,va_mask1, tr_mask2,va_mask2)  = train_test_split(data, time, label, mask1, mask2, test_size=0.20, random_state=seed) \n","\n","    max_valid = -99\n","    stop_flag = 0\n","\n","    ### TRAINING - MAIN\n","    print( \"MAIN TRAINING ...\")\n","    print( \"EVALUATION TIMES: \" + str(eval_time))\n","\n","    avg_loss = 0\n","    for itr in range(iteration):\n","        if stop_flag > 5: #for faster early stopping\n","            break\n","        else:\n","            x_mb, k_mb, t_mb, m1_mb, m2_mb = f_get_minibatch(mb_size, tr_data, tr_label, tr_time, tr_mask1, tr_mask2)\n","            DATA = (x_mb, k_mb, t_mb)\n","            MASK = (m1_mb, m2_mb)\n","            PARAMETERS = (alpha, beta, gamma)\n","            _, loss_curr = model.train(DATA, MASK, PARAMETERS, keep_prob, lr_train)\n","            avg_loss += loss_curr/1000\n","                \n","            if (itr+1)%1000 == 0:\n","                print('|| ITR: ' + str('%04d' % (itr + 1)) + ' | Loss: ' + colored(str('%.4f' %(avg_loss)), 'yellow' , attrs=['bold']))\n","                avg_loss = 0\n","\n","            ### VALIDATION  (based on average C-index of our interest)\n","            if (itr+1)%1000 == 0:\n","                ### PREDICTION\n","                pred = model.predict(va_data)\n","\n","                ### EVALUATION\n","                va_result1 = np.zeros([num_Event, len(eval_time)])\n","\n","                for t, t_time in enumerate(eval_time):\n","                    eval_horizon = int(t_time)\n","\n","                    if eval_horizon >= num_Category:\n","                        print('ERROR: evaluation horizon is out of range')\n","                        va_result1[:, t] = va_result2[:, t] = -1\n","                    else:\n","                        risk = np.sum(pred[:,:,:(eval_horizon+1)], axis=2) #risk score until eval_time\n","                        for k in range(num_Event):\n","                            va_result1[k, t] = c_index(risk[:,k], va_time, (va_label[:,0] == k+1).astype(int), eval_horizon) #-1 for no event (not comparable)\n","                tmp_valid = np.mean(va_result1)\n","\n","\n","                if tmp_valid >  max_valid:\n","                    stop_flag = 0\n","                    max_valid = tmp_valid\n","                    print( 'updated.... average c-index = ' + str('%.4f' %(tmp_valid)))\n","                else:\n","                    stop_flag += 1\n","\n","    return max_valid\n","\n","\n","#IMPORT_DATA\n","\n","def f_get_Normalization(X, norm_mode):\n","    num_Patient, num_Feature = np.shape(X)\n","\n","    if norm_mode == 'standard': #zero mean unit variance\n","        for j in range(num_Feature):\n","            if np.std(X[:,j]) != 0:\n","                X[:,j] = (X[:,j] - np.mean(X[:, j]))/np.std(X[:,j])\n","            else:\n","                X[:,j] = (X[:,j] - np.mean(X[:, j]))\n","    elif norm_mode == 'normal': #min-max normalization\n","        for j in range(num_Feature):\n","            X[:,j] = (X[:,j] - np.min(X[:,j]))/(np.max(X[:,j]) - np.min(X[:,j]))\n","    else:\n","        print(\"INPUT MODE ERROR!\")\n","\n","    return X\n","\n","### MASK FUNCTIONS\n","def f_get_fc_mask2(time, label, num_Event, num_Category):\n","    mask = np.zeros([np.shape(time)[0], num_Event, num_Category]) # for the first loss function\n","    for i in range(np.shape(time)[0]):\n","        if label[i,0] != 0:  #not censored\n","            mask[i,int(label[i,0]-1),int(time[i,0])] = 1\n","        else: #label[i,2]==0: censored\n","            mask[i,:,int(time[i,0]+1):] =  1 #fill 1 until from the censoring time (to get 1 - \\sum F)\n","    return mask\n","\n","def f_get_fc_mask3(time, meas_time, num_Category):\n","    mask = np.zeros([np.shape(time)[0], num_Category]) # for the first loss function\n","    if np.shape(meas_time):  #lonogitudinal measurements\n","        for i in range(np.shape(time)[0]):\n","            t1 = int(meas_time[i, 0]) # last measurement time\n","            t2 = int(time[i, 0]) # censoring/event time\n","            mask[i,(t1+1):(t2+1)] = 1  #this excludes the last measurement time and includes the event time\n","    else:                    #single measurement\n","        for i in range(np.shape(time)[0]):\n","            t = int(time[i, 0]) # censoring/event time\n","            mask[i,:(t+1)] = 1  #this excludes the last measurement time and includes the event time\n","    return mask\n","\n","\n","#MAIN_RANDOMSEARCH (PART 1)\n","\n","### SAVE AND LOAD HYPERPARAMETERS\n","def save_logging(dictionary, log_name): # this saves the current hyperparameters\n","    with open(log_name, 'w') as f:\n","        for key, value in dictionary.items():\n","            f.write('%s:%s\\n' % (key, value))\n","\n","def load_logging(filename): # this open can calls the saved hyperparameters\n","    data = dict()\n","    with open(filename) as f:\n","        def is_float(input):\n","            try:\n","                num = float(input)\n","            except ValueError:\n","                return False\n","            return True\n","\n","        for line in f.readlines():\n","            if ':' in line:\n","                key,value = line.strip().split(':', 1)\n","                if value.isdigit():\n","                    data[key] = int(value)\n","                elif is_float(value):\n","                    data[key] = float(value)\n","                elif value == 'None':\n","                    data[key] = None\n","                else:\n","                    data[key] = value\n","            else:\n","                pass # deal with bad lines of text here    \n","    return data\n","\n","\n","#own functions\n","\n","def train_save_deephit(DATA, MASK, in_parser, eval_time):\n","    ##### DATA & MASK\n","    (data, time, label)  = DATA\n","    (mask1, mask2)       = MASK\n","\n","    x_dim                       = np.shape(data)[1]\n","    _, num_Event, num_Category  = np.shape(mask1)  # dim of mask1: [subj, Num_Event, Num_Category]\n","    \n","    ACTIVATION_FN               = {'relu': tf.nn.relu, 'elu': tf.nn.elu, 'tanh': tf.nn.tanh}\n","\n","    ##### HYPER-PARAMETERS\n","    mb_size                     = in_parser['mb_size']\n","\n","    iteration                   = in_parser['iteration']\n","\n","    keep_prob                   = in_parser['keep_prob']\n","    lr_train                    = in_parser['lr_train']\n","\n","\n","    alpha                       = in_parser['alpha']  #for log-likelihood loss\n","    beta                        = in_parser['beta']  #for ranking loss\n","    gamma                       = in_parser['gamma']  #for RNN-prediction loss\n","    parameter_name              = 'a' + str('%02.0f' %(10*alpha)) + 'b' + str('%02.0f' %(10*beta)) + 'c' + str('%02.0f' %(10*gamma))\n","\n","    initial_W                   = tf.contrib.layers.xavier_initializer()\n","\n","    ##### MAKE DICTIONARIES\n","    # INPUT DIMENSIONS\n","    input_dims                  = { 'x_dim'         : x_dim,\n","                                    'num_Event'     : num_Event,\n","                                    'num_Category'  : num_Category}\n","    # NETWORK HYPER-PARMETERS\n","    network_settings            = { 'h_dim_shared'       : in_parser['h_dim_shared'],\n","                                    'num_layers_shared'  : in_parser['num_layers_shared'],\n","                                    'h_dim_CS'           : in_parser['h_dim_CS'],\n","                                    'num_layers_CS'      : in_parser['num_layers_CS'],\n","                                    'active_fn'          : ACTIVATION_FN[in_parser['active_fn']],\n","                                    'initial_W'          : initial_W }\n","\n","    ##### CREATE DEEPHIT NETWORK\n","    tf.reset_default_graph()\n","\n","    config = tf.ConfigProto()\n","    config.gpu_options.allow_growth = True\n","    sess = tf.Session(config=config)\n","\n","    model = Model_DeepHit(sess, \"DeepHit\", input_dims, network_settings)\n","    saver = tf.train.Saver()\n","\n","    sess.run(tf.global_variables_initializer())\n","\n","    ### TRAINING-TESTING SPLIT\n","    (tr_data,te_data, tr_time,te_time, tr_label,te_label, \n","     tr_mask1,te_mask1, tr_mask2,te_mask2)  = train_test_split(data, time, label, mask1, mask2, test_size=0.20, random_state=seed)\n","\n","    stop_flag = 0\n","\n","    avg_loss = 0\n","    for itr in range(iteration):\n","        if stop_flag > 5: #for faster early stopping\n","            break\n","        else:\n","            x_mb, k_mb, t_mb, m1_mb, m2_mb = f_get_minibatch(mb_size, tr_data, tr_label, tr_time, tr_mask1, tr_mask2)\n","            DATA = (x_mb, k_mb, t_mb)\n","            MASK = (m1_mb, m2_mb)\n","            PARAMETERS = (alpha, beta, gamma)\n","            _, loss_curr = model.train(DATA, MASK, PARAMETERS, keep_prob, lr_train)\n","                    \n","    saver.save(sess, \"/content/drive/My Drive/deephitmodel/model\")\n","\n","def import_dataset_OWN(norm_mode='standard'):\n","    df = pd.read_csv('loandata.csv', sep=',')\n","    \n","    label           = np.asarray(df[['label']])\n","    time            = np.asarray(df[['time']])\n","    data            = np.asarray(df[cols])\n","    data            = f_get_Normalization(data, norm_mode)\n","\n","    num_Category    = int(np.max(time) * 1.2)  #to have enough time-horizon\n","    num_Event       = int(len(np.unique(label)) - 1) #only count the number of events (do not count censoring as an event)\n","\n","    x_dim           = np.shape(data)[1]\n","\n","    mask1           = f_get_fc_mask2(time, label, num_Event, num_Category)\n","    mask2           = f_get_fc_mask3(time, -1, num_Category)\n","\n","    DIM             = (x_dim)\n","    DATA            = (data, time, label)\n","    MASK            = (mask1, mask2)\n","\n","    return DIM, DATA, MASK\n","\n","def import_dataset_OWN_CR(norm_mode='standard'):\n","    df = pd.read_csv('loandataCR.csv', sep=',')\n","    \n","    label           = np.asarray(df[['label']])\n","    time            = np.asarray(df[['time']])\n","    data            = np.asarray(df[cols])\n","    data            = f_get_Normalization(data, norm_mode)\n","\n","    num_Category    = int(np.max(time) * 1.2)  #to have enough time-horizon\n","    num_Event       = int(len(np.unique(label)) - 1) #only count the number of events (do not count censoring as an event)\n","\n","    x_dim           = np.shape(data)[1]\n","\n","    mask1           = f_get_fc_mask2(time, label, num_Event, num_Category)\n","    mask2           = f_get_fc_mask3(time, -1, num_Category)\n","\n","    DIM             = (x_dim)\n","    DATA            = (data, time, label)\n","    MASK            = (mask1, mask2)\n","\n","    return DIM, DATA, MASK\n","\n","def import_dataset_HYP(norm_mode='standard'):\n","    df = pd.read_csv('loandataHT.csv', sep=',')\n","    \n","    label           = np.asarray(df[['label']])\n","    time            = np.asarray(df[['time']])\n","    data            = np.asarray(df[cols])\n","    data            = f_get_Normalization(data, norm_mode)\n","\n","    num_Category    = int(np.max(time) * 1.2)\n","    num_Event       = int(len(np.unique(label)) - 1)\n","\n","    x_dim           = np.shape(data)[1]\n","\n","    mask1           = f_get_fc_mask2(time, label, num_Event, num_Category)\n","    mask2           = f_get_fc_mask3(time, -1, num_Category)\n","\n","    DIM             = (x_dim)\n","    DATA            = (data, time, label)\n","    MASK            = (mask1, mask2)\n","\n","    return DIM, DATA, MASK"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_seAwNLrtzox"},"source":["### Part 2: Hyperparameter tuning"]},{"cell_type":"markdown","metadata":{"id":"BJ6n1-dm2-Qb"},"source":["In this part, hyperparameter tuning is performed for the datasets by implementing a random search. For this purpose, the datasets \"loandataHT.csv\" and \"newdataHT.csv\" are loaded, and all available variables are used. The sets of possible values for each hyperparameter of interest are then defined. The number of times of randomly choosing values for each hyperparameter from the previously defined sets and training a DeepHit model based on these values is defined by the variable RS_NR. The hyperparameters with the best performance (here: the average concordance index recorded at 24, 48, and 72 months after loan issuance for both events) on the test set (containing 20% of the respective dataset) are then used for all subsequent DeepHit models in this study."]},{"cell_type":"code","metadata":{"id":"AGbB2tVk2UBQ","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1575475169145,"user_tz":-60,"elapsed":1611447,"user":{"displayName":"Gabriel Blumenstock","photoUrl":"","userId":"15396447649265516687"}},"outputId":"c8fdcc5d-08fd-4a36-e045-7896e6b88681"},"source":["#HYPERPARAMETER TUNING - DATASET 1\n","\n","cols = [\"int.rate\", \"orig.upb\", \"fico.score\", \"dti.r\",  \"ltv.r\", \"bal.repaid\", \"t.act.12m\", \"t.del.30d.12m\", \"t.del.60d.12m\", \"hpi.st.d.t.o\", \"hpi.zip.o\", \"hpi.zip.d.t.o\", \"ppi.c.FRMA\", \"TB10Y.d.t.o\", \"FRMA30Y.d.t.o\", \"ppi.o.FRMA\", \"equity.est\", \"hpi.st.log12m\", \"hpi.r.st.us\", \"hpi.r.zip.st\", \"st.unemp.r12m\", \"st.unemp.r3m\", \"TB10Y.r12m\", \"T10Y3MM\", \"T10Y3MM.r12m\"]\n","\n","!cp \"/content/drive/My Drive/datasets/loandataHT.csv\" \"loandataHT.csv\"\n","(x_dim), (data, time, label), (mask1, mask2) = import_dataset_HYP(norm_mode = 'standard')\n","EVAL_TIMES = [24, 48, 72]\n","DATA = (data, time, label)\n","MASK = (mask1, mask2)\n","\n","def get_random_hyperparameters():\n","\n","    SET_LAYERS        = [1,2,3,5]\n","    SET_NODES         = [50, 100, 200, 300]\n","    SET_ACTIVATION_FN = ['relu', 'tanh'] \n","    SET_BETA          = [0.1, 0.5, 1.0, 3.0, 5.0]\n","\n","    new_parser = {'mb_size': 128,\n","                 'iteration': 3000,\n","                 'keep_prob': 0.6,\n","                 'lr_train': 1e-4,\n","                 'h_dim_shared': SET_NODES[np.random.randint(len(SET_NODES))],\n","                 'h_dim_CS': SET_NODES[np.random.randint(len(SET_NODES))],\n","                 'num_layers_shared':SET_LAYERS[np.random.randint(len(SET_LAYERS))],\n","                 'num_layers_CS':SET_LAYERS[np.random.randint(len(SET_LAYERS))],\n","                 'active_fn': SET_ACTIVATION_FN[np.random.randint(len(SET_ACTIVATION_FN))],\n","                 'alpha':1.0,\n","                 'beta':SET_BETA[np.random.randint(len(SET_BETA))],\n","                 'gamma':0,\n","                 }\n","    \n","    return new_parser\n","\n","RS_NR                = 30\n","max_valid = 0.\n","\n","for r_itr in range(RS_NR):\n","    print('Random search... itr: ' + str(r_itr))\n","    new_parser = get_random_hyperparameters()\n","    print(new_parser)\n","\n","    tmp_max = get_valid_performance(DATA, MASK, new_parser, EVAL_TIMES)\n","\n","    print('Current: ' + str(tmp_max))\n","\n","    if tmp_max > max_valid:\n","        max_valid = tmp_max\n","        max_parser = new_parser\n","        #save_logging(max_parser, \"/content/drive/My Drive/hyp_D1.txt\")\n","\n","    print('Current best: ' + str(max_valid))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Random search... itr: 0\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 200, 'h_dim_CS': 50, 'num_layers_shared': 3, 'num_layers_CS': 3, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 5.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m122.3980\u001b[0m\n","updated.... average c-index = 0.9277\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m92.5101\u001b[0m\n","updated.... average c-index = 0.9418\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m79.4223\u001b[0m\n","updated.... average c-index = 0.9535\n","Current: 0.9534890118396925\n","Current best: 0.9534890118396925\n","Random search... itr: 1\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 200, 'num_layers_shared': 3, 'num_layers_CS': 2, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 3.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m57.8832\u001b[0m\n","updated.... average c-index = 0.9264\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m38.6292\u001b[0m\n","updated.... average c-index = 0.9295\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m32.6598\u001b[0m\n","Current: 0.9294820432237958\n","Current best: 0.9534890118396925\n","Random search... itr: 2\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 300, 'num_layers_shared': 5, 'num_layers_CS': 1, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 1.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m19.1349\u001b[0m\n","updated.... average c-index = 0.9286\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m13.4472\u001b[0m\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m12.0834\u001b[0m\n","Current: 0.9285942019820875\n","Current best: 0.9534890118396925\n","Random search... itr: 3\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 200, 'num_layers_shared': 2, 'num_layers_CS': 1, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 1.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m19.5443\u001b[0m\n","updated.... average c-index = 0.9400\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m14.2051\u001b[0m\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m12.4810\u001b[0m\n","Current: 0.9399822518214428\n","Current best: 0.9534890118396925\n","Random search... itr: 4\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 300, 'h_dim_CS': 50, 'num_layers_shared': 2, 'num_layers_CS': 3, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 5.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m108.2518\u001b[0m\n","updated.... average c-index = 0.9116\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m77.4250\u001b[0m\n","updated.... average c-index = 0.9426\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m65.0725\u001b[0m\n","updated.... average c-index = 0.9636\n","Current: 0.9636283555011662\n","Current best: 0.9636283555011662\n","Random search... itr: 5\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 200, 'h_dim_CS': 300, 'num_layers_shared': 2, 'num_layers_CS': 5, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 5.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m85.6418\u001b[0m\n","updated.... average c-index = 0.9231\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m65.1431\u001b[0m\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m56.4361\u001b[0m\n","updated.... average c-index = 0.9236\n","Current: 0.9236142759587787\n","Current best: 0.9636283555011662\n","Random search... itr: 6\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 200, 'num_layers_shared': 3, 'num_layers_CS': 1, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 0.1, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m6.0728\u001b[0m\n","updated.... average c-index = 0.9042\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m4.8415\u001b[0m\n","updated.... average c-index = 0.9045\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m4.4494\u001b[0m\n","Current: 0.9044683450976652\n","Current best: 0.9636283555011662\n","Random search... itr: 7\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 300, 'h_dim_CS': 50, 'num_layers_shared': 1, 'num_layers_CS': 1, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 0.5, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m12.7229\u001b[0m\n","updated.... average c-index = 0.9148\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m8.6382\u001b[0m\n","updated.... average c-index = 0.9274\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m7.4974\u001b[0m\n","updated.... average c-index = 0.9275\n","Current: 0.9275347518873803\n","Current best: 0.9636283555011662\n","Random search... itr: 8\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 200, 'h_dim_CS': 50, 'num_layers_shared': 1, 'num_layers_CS': 2, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 1.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m24.4147\u001b[0m\n","updated.... average c-index = 0.9207\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m16.5293\u001b[0m\n","updated.... average c-index = 0.9359\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m13.9878\u001b[0m\n","updated.... average c-index = 0.9432\n","Current: 0.9432074942331644\n","Current best: 0.9636283555011662\n","Random search... itr: 9\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 50, 'num_layers_shared': 2, 'num_layers_CS': 1, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 1.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m23.3847\u001b[0m\n","updated.... average c-index = 0.9367\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m17.4218\u001b[0m\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m15.2571\u001b[0m\n","Current: 0.9366614142904809\n","Current best: 0.9636283555011662\n","Random search... itr: 10\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 100, 'num_layers_shared': 1, 'num_layers_CS': 1, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 0.5, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m12.9009\u001b[0m\n","updated.... average c-index = 0.9230\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m9.1744\u001b[0m\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m8.0403\u001b[0m\n","updated.... average c-index = 0.9261\n","Current: 0.9260713638981258\n","Current best: 0.9636283555011662\n","Random search... itr: 11\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 50, 'num_layers_shared': 3, 'num_layers_CS': 2, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 5.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m125.3767\u001b[0m\n","updated.... average c-index = 0.8976\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m87.4548\u001b[0m\n","updated.... average c-index = 0.9225\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m72.9648\u001b[0m\n","updated.... average c-index = 0.9309\n","Current: 0.9309027365564249\n","Current best: 0.9636283555011662\n","Random search... itr: 12\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 200, 'h_dim_CS': 50, 'num_layers_shared': 3, 'num_layers_CS': 5, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 0.5, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m18.4190\u001b[0m\n","updated.... average c-index = 0.7655\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m14.8802\u001b[0m\n","updated.... average c-index = 0.7868\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m13.2670\u001b[0m\n","updated.... average c-index = 0.8217\n","Current: 0.8216879641113602\n","Current best: 0.9636283555011662\n","Random search... itr: 13\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 200, 'num_layers_shared': 1, 'num_layers_CS': 3, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 0.1, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m5.8760\u001b[0m\n","updated.... average c-index = 0.8801\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m4.7524\u001b[0m\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m4.3601\u001b[0m\n","Current: 0.8800691386741284\n","Current best: 0.9636283555011662\n","Random search... itr: 14\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 100, 'num_layers_shared': 1, 'num_layers_CS': 2, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 0.1, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m6.4543\u001b[0m\n","updated.... average c-index = 0.8825\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m5.1392\u001b[0m\n","updated.... average c-index = 0.9167\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m4.6853\u001b[0m\n","Current: 0.9166608598049963\n","Current best: 0.9636283555011662\n","Random search... itr: 15\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 200, 'h_dim_CS': 100, 'num_layers_shared': 3, 'num_layers_CS': 2, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 3.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m65.5980\u001b[0m\n","updated.... average c-index = 0.9254\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m45.8781\u001b[0m\n","updated.... average c-index = 0.9410\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m38.7599\u001b[0m\n","updated.... average c-index = 0.9420\n","Current: 0.9420018788527224\n","Current best: 0.9636283555011662\n","Random search... itr: 16\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 200, 'h_dim_CS': 300, 'num_layers_shared': 1, 'num_layers_CS': 5, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 0.5, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m12.7676\u001b[0m\n","updated.... average c-index = 0.9203\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m8.9708\u001b[0m\n","updated.... average c-index = 0.9360\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m7.5818\u001b[0m\n","Current: 0.9359644938886335\n","Current best: 0.9636283555011662\n","Random search... itr: 17\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 100, 'num_layers_shared': 3, 'num_layers_CS': 2, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 0.5, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m13.5778\u001b[0m\n","updated.... average c-index = 0.9242\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m10.7654\u001b[0m\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m9.4919\u001b[0m\n","Current: 0.9241862532613089\n","Current best: 0.9636283555011662\n","Random search... itr: 18\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 50, 'num_layers_shared': 1, 'num_layers_CS': 3, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 0.5, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m17.0912\u001b[0m\n","updated.... average c-index = 0.9146\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m13.2536\u001b[0m\n","updated.... average c-index = 0.9269\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m11.5250\u001b[0m\n","updated.... average c-index = 0.9295\n","Current: 0.9294903866462899\n","Current best: 0.9636283555011662\n","Random search... itr: 19\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 200, 'h_dim_CS': 50, 'num_layers_shared': 2, 'num_layers_CS': 2, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 0.1, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m6.6009\u001b[0m\n","updated.... average c-index = 0.8965\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m5.4226\u001b[0m\n","updated.... average c-index = 0.9194\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m4.9787\u001b[0m\n","Current: 0.9194429789371225\n","Current best: 0.9636283555011662\n","Random search... itr: 20\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 300, 'num_layers_shared': 2, 'num_layers_CS': 2, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 0.1, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m5.6979\u001b[0m\n","updated.... average c-index = 0.8832\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m4.6169\u001b[0m\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m4.2635\u001b[0m\n","Current: 0.8831842192031255\n","Current best: 0.9636283555011662\n","Random search... itr: 21\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 300, 'h_dim_CS': 200, 'num_layers_shared': 1, 'num_layers_CS': 2, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 5.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m74.2670\u001b[0m\n","updated.... average c-index = 0.9291\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m44.5610\u001b[0m\n","updated.... average c-index = 0.9421\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m36.7154\u001b[0m\n","Current: 0.9420594440912343\n","Current best: 0.9636283555011662\n","Random search... itr: 22\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 200, 'h_dim_CS': 300, 'num_layers_shared': 3, 'num_layers_CS': 2, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 5.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m78.0799\u001b[0m\n","updated.... average c-index = 0.9289\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m56.6315\u001b[0m\n","updated.... average c-index = 0.9294\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m49.4341\u001b[0m\n","Current: 0.929411039322277\n","Current best: 0.9636283555011662\n","Random search... itr: 23\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 300, 'h_dim_CS': 200, 'num_layers_shared': 5, 'num_layers_CS': 3, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 0.1, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m6.1747\u001b[0m\n","updated.... average c-index = 0.8121\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m5.1948\u001b[0m\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m4.7585\u001b[0m\n","Current: 0.8121468640627431\n","Current best: 0.9636283555011662\n","Random search... itr: 24\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 100, 'num_layers_shared': 1, 'num_layers_CS': 2, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 5.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m84.7291\u001b[0m\n","updated.... average c-index = 0.9392\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m58.5460\u001b[0m\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m50.0214\u001b[0m\n","Current: 0.9391870673695326\n","Current best: 0.9636283555011662\n","Random search... itr: 25\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 200, 'h_dim_CS': 200, 'num_layers_shared': 2, 'num_layers_CS': 1, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 1.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m18.0275\u001b[0m\n","updated.... average c-index = 0.9225\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m13.3714\u001b[0m\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m11.8134\u001b[0m\n","Current: 0.9224945293160359\n","Current best: 0.9636283555011662\n","Random search... itr: 26\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 200, 'num_layers_shared': 3, 'num_layers_CS': 3, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 0.1, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m6.3068\u001b[0m\n","updated.... average c-index = 0.9003\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m5.1447\u001b[0m\n","updated.... average c-index = 0.9160\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m4.6988\u001b[0m\n","Current: 0.9160029531408731\n","Current best: 0.9636283555011662\n","Random search... itr: 27\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 300, 'num_layers_shared': 1, 'num_layers_CS': 3, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 3.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m49.9252\u001b[0m\n","updated.... average c-index = 0.9457\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m32.0294\u001b[0m\n","updated.... average c-index = 0.9484\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m26.4605\u001b[0m\n","Current: 0.9483680832631246\n","Current best: 0.9636283555011662\n","Random search... itr: 28\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 200, 'h_dim_CS': 100, 'num_layers_shared': 5, 'num_layers_CS': 1, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 0.1, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m6.1894\u001b[0m\n","updated.... average c-index = 0.8310\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m5.1017\u001b[0m\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m4.6455\u001b[0m\n","Current: 0.8309790308038703\n","Current best: 0.9636283555011662\n","Random search... itr: 29\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 100, 'num_layers_shared': 5, 'num_layers_CS': 1, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 5.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m93.3591\u001b[0m\n","updated.... average c-index = 0.9273\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m62.6139\u001b[0m\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m53.6857\u001b[0m\n","Current: 0.9272707417209102\n","Current best: 0.9636283555011662\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nmHXMld8off2","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1575477103648,"user_tz":-60,"elapsed":1875116,"user":{"displayName":"Gabriel Blumenstock","photoUrl":"","userId":"15396447649265516687"}},"outputId":"9aea32e5-d775-4e7d-8e4a-56d1a3c79e14"},"source":["#HYPERPARAMETER TUNING - Dataset 2\n","\n","cols = [\"fico.score\", \"dti.r\", \"orig.upb\", \"ltv.r\", \"int.rate\", \"t.act.12m\", \"t.del.30d.12m\", \"t.del.60d.12m\", \"bal.repaid\", \"hpi.st.d.t.o\", \"hpi.r.st.us\", \"FRMA30Y.d.t.o\", \"ppi.c.FRMA\", \"ppi.o.FRMA\", \"st.unemp.r12m\", \"st.unemp.r3m\", \"TB10Y.d.t.o\", \"TB10Y.r12m\", \"T10Y3MM\", \"T10Y3MM.r12m\"]\n","\n","!cp \"/content/drive/My Drive/datasets/newdataHT.csv\" \"loandataHT.csv\"\n","(x_dim), (data, time, label), (mask1, mask2) = import_dataset_HYP(norm_mode = 'standard')\n","EVAL_TIMES = [24, 48, 72]\n","DATA = (data, time, label)\n","MASK = (mask1, mask2)\n","\n","def get_random_hyperparameters():\n","\n","    SET_LAYERS        = [1,2,3,5]\n","    SET_NODES         = [50, 100, 200, 300]\n","    SET_ACTIVATION_FN = ['relu', 'tanh'] \n","    SET_BETA          = [0.1, 0.5, 1.0, 3.0, 5.0]\n","\n","    new_parser = {'mb_size': 128,\n","                 'iteration': 3000,\n","                 'keep_prob': 0.6,\n","                 'lr_train': 1e-4,\n","                 'h_dim_shared': SET_NODES[np.random.randint(len(SET_NODES))],\n","                 'h_dim_CS': SET_NODES[np.random.randint(len(SET_NODES))],\n","                 'num_layers_shared':SET_LAYERS[np.random.randint(len(SET_LAYERS))],\n","                 'num_layers_CS':SET_LAYERS[np.random.randint(len(SET_LAYERS))],\n","                 'active_fn': SET_ACTIVATION_FN[np.random.randint(len(SET_ACTIVATION_FN))],\n","                 'alpha':1.0,\n","                 'beta':SET_BETA[np.random.randint(len(SET_BETA))],\n","                 'gamma':0,\n","                 }\n","    \n","    return new_parser\n","\n","RS_NR                = 30\n","max_valid = 0.\n","\n","for r_itr in range(RS_NR):\n","    print('Random search... itr: ' + str(r_itr))\n","    new_parser = get_random_hyperparameters()\n","    print(new_parser)\n","\n","    tmp_max = get_valid_performance(DATA, MASK, new_parser, EVAL_TIMES)\n","\n","    print('Current: ' + str(tmp_max))\n","\n","    if tmp_max > max_valid:\n","        max_valid = tmp_max\n","        max_parser = new_parser\n","        #save_logging(max_parser, \"/content/drive/My Drive/hyp_D2.txt\")\n","\n","    print('Current best: ' + str(max_valid))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Random search... itr: 0\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 200, 'h_dim_CS': 100, 'num_layers_shared': 5, 'num_layers_CS': 2, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 3.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m69.0528\u001b[0m\n","updated.... average c-index = 0.9179\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m52.8476\u001b[0m\n","updated.... average c-index = 0.9361\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m45.9812\u001b[0m\n","updated.... average c-index = 0.9366\n","Current: 0.9365878024789872\n","Current best: 0.9365878024789872\n","Random search... itr: 1\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 50, 'num_layers_shared': 1, 'num_layers_CS': 1, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 3.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m63.7027\u001b[0m\n","updated.... average c-index = 0.9256\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m47.7771\u001b[0m\n","updated.... average c-index = 0.9394\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m41.1262\u001b[0m\n","updated.... average c-index = 0.9427\n","Current: 0.9427001356986128\n","Current best: 0.9427001356986128\n","Random search... itr: 2\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 200, 'num_layers_shared': 3, 'num_layers_CS': 2, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 0.1, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m5.8426\u001b[0m\n","updated.... average c-index = 0.9020\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m5.0000\u001b[0m\n","updated.... average c-index = 0.9312\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m4.6720\u001b[0m\n","Current: 0.9312413458324102\n","Current best: 0.9427001356986128\n","Random search... itr: 3\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 200, 'h_dim_CS': 300, 'num_layers_shared': 1, 'num_layers_CS': 5, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 5.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m89.9008\u001b[0m\n","updated.... average c-index = 0.9557\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m62.8391\u001b[0m\n","updated.... average c-index = 0.9582\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m53.8541\u001b[0m\n","Current: 0.9582201292880401\n","Current best: 0.9582201292880401\n","Random search... itr: 4\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 300, 'num_layers_shared': 3, 'num_layers_CS': 5, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 0.5, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m14.3438\u001b[0m\n","updated.... average c-index = 0.8749\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m11.3424\u001b[0m\n","updated.... average c-index = 0.9176\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m10.0364\u001b[0m\n","Current: 0.9175567224530933\n","Current best: 0.9582201292880401\n","Random search... itr: 5\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 200, 'h_dim_CS': 100, 'num_layers_shared': 5, 'num_layers_CS': 3, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 1.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m22.6198\u001b[0m\n","updated.... average c-index = 0.9211\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m18.5874\u001b[0m\n","updated.... average c-index = 0.9240\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m17.0746\u001b[0m\n","Current: 0.9240228486178657\n","Current best: 0.9582201292880401\n","Random search... itr: 6\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 200, 'h_dim_CS': 300, 'num_layers_shared': 3, 'num_layers_CS': 3, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 0.5, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m12.1348\u001b[0m\n","updated.... average c-index = 0.9362\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m10.2647\u001b[0m\n","updated.... average c-index = 0.9367\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m9.5989\u001b[0m\n","updated.... average c-index = 0.9370\n","Current: 0.9369878034763438\n","Current best: 0.9582201292880401\n","Random search... itr: 7\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 300, 'h_dim_CS': 300, 'num_layers_shared': 2, 'num_layers_CS': 2, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 0.1, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m5.2521\u001b[0m\n","updated.... average c-index = 0.9246\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m4.6242\u001b[0m\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m4.4273\u001b[0m\n","Current: 0.9245600759238015\n","Current best: 0.9582201292880401\n","Random search... itr: 8\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 200, 'num_layers_shared': 2, 'num_layers_CS': 2, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 1.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m24.2877\u001b[0m\n","updated.... average c-index = 0.9295\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m19.0248\u001b[0m\n","updated.... average c-index = 0.9310\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m16.8104\u001b[0m\n","Current: 0.9310341922441974\n","Current best: 0.9582201292880401\n","Random search... itr: 9\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 300, 'h_dim_CS': 200, 'num_layers_shared': 2, 'num_layers_CS': 3, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 5.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m91.6285\u001b[0m\n","updated.... average c-index = 0.9498\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m67.0158\u001b[0m\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m57.9950\u001b[0m\n","Current: 0.9497654277968044\n","Current best: 0.9582201292880401\n","Random search... itr: 10\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 100, 'num_layers_shared': 2, 'num_layers_CS': 2, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 3.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m70.3042\u001b[0m\n","updated.... average c-index = 0.9220\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m56.9480\u001b[0m\n","updated.... average c-index = 0.9336\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m50.5026\u001b[0m\n","updated.... average c-index = 0.9365\n","Current: 0.9364653959376915\n","Current best: 0.9582201292880401\n","Random search... itr: 11\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 200, 'num_layers_shared': 1, 'num_layers_CS': 5, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 5.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m87.6924\u001b[0m\n","updated.... average c-index = 0.9241\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m70.0000\u001b[0m\n","updated.... average c-index = 0.9318\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m62.9356\u001b[0m\n","Current: 0.9318109955567251\n","Current best: 0.9582201292880401\n","Random search... itr: 12\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 300, 'num_layers_shared': 5, 'num_layers_CS': 1, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 0.1, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m5.5575\u001b[0m\n","updated.... average c-index = 0.9056\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m4.7611\u001b[0m\n","updated.... average c-index = 0.9067\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m4.4976\u001b[0m\n","updated.... average c-index = 0.9172\n","Current: 0.9171593066872133\n","Current best: 0.9582201292880401\n","Random search... itr: 13\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 200, 'num_layers_shared': 1, 'num_layers_CS': 3, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 5.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m101.8689\u001b[0m\n","updated.... average c-index = 0.9376\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m74.4625\u001b[0m\n","updated.... average c-index = 0.9467\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m62.6061\u001b[0m\n","Current: 0.9467316423895998\n","Current best: 0.9582201292880401\n","Random search... itr: 14\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 50, 'num_layers_shared': 2, 'num_layers_CS': 5, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 3.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m70.3136\u001b[0m\n","updated.... average c-index = 0.9185\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m56.1853\u001b[0m\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m51.2150\u001b[0m\n","Current: 0.9184612872803104\n","Current best: 0.9582201292880401\n","Random search... itr: 15\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 200, 'num_layers_shared': 3, 'num_layers_CS': 2, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 0.5, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m13.6865\u001b[0m\n","updated.... average c-index = 0.9250\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m10.8805\u001b[0m\n","updated.... average c-index = 0.9260\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m9.7186\u001b[0m\n","Current: 0.9259788864913451\n","Current best: 0.9582201292880401\n","Random search... itr: 16\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 200, 'h_dim_CS': 200, 'num_layers_shared': 5, 'num_layers_CS': 2, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 1.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m21.2394\u001b[0m\n","updated.... average c-index = 0.9307\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m17.4071\u001b[0m\n","updated.... average c-index = 0.9347\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m15.9040\u001b[0m\n","Current: 0.9346628523739678\n","Current best: 0.9582201292880401\n","Random search... itr: 17\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 50, 'num_layers_shared': 2, 'num_layers_CS': 3, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 5.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m150.1064\u001b[0m\n","updated.... average c-index = 0.6535\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m123.6993\u001b[0m\n","updated.... average c-index = 0.8397\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m109.5794\u001b[0m\n","updated.... average c-index = 0.9053\n","Current: 0.9052842966902638\n","Current best: 0.9582201292880401\n","Random search... itr: 18\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 200, 'h_dim_CS': 100, 'num_layers_shared': 2, 'num_layers_CS': 1, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 1.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m25.9604\u001b[0m\n","updated.... average c-index = 0.8469\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m19.6120\u001b[0m\n","updated.... average c-index = 0.9403\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m16.7964\u001b[0m\n","updated.... average c-index = 0.9445\n","Current: 0.9445201602517764\n","Current best: 0.9582201292880401\n","Random search... itr: 19\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 50, 'h_dim_CS': 300, 'num_layers_shared': 2, 'num_layers_CS': 5, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 5.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m89.0218\u001b[0m\n","updated.... average c-index = 0.9278\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m71.6269\u001b[0m\n","updated.... average c-index = 0.9363\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m64.7512\u001b[0m\n","updated.... average c-index = 0.9377\n","Current: 0.9377128184218478\n","Current best: 0.9582201292880401\n","Random search... itr: 20\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 300, 'h_dim_CS': 50, 'num_layers_shared': 1, 'num_layers_CS': 1, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 1.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m22.0081\u001b[0m\n","updated.... average c-index = 0.9398\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m15.4919\u001b[0m\n","updated.... average c-index = 0.9468\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m13.4038\u001b[0m\n","Current: 0.9467537685917559\n","Current best: 0.9582201292880401\n","Random search... itr: 21\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 50, 'num_layers_shared': 2, 'num_layers_CS': 3, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 0.5, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m14.1512\u001b[0m\n","updated.... average c-index = 0.9210\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m11.7755\u001b[0m\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m10.9306\u001b[0m\n","Current: 0.9209628006619229\n","Current best: 0.9582201292880401\n","Random search... itr: 22\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 300, 'num_layers_shared': 3, 'num_layers_CS': 5, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 0.1, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m5.8526\u001b[0m\n","updated.... average c-index = 0.9108\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m5.1311\u001b[0m\n","updated.... average c-index = 0.9188\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m4.8439\u001b[0m\n","updated.... average c-index = 0.9212\n","Current: 0.9212493460389619\n","Current best: 0.9582201292880401\n","Random search... itr: 23\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 100, 'h_dim_CS': 100, 'num_layers_shared': 3, 'num_layers_CS': 1, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 0.5, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m17.2331\u001b[0m\n","updated.... average c-index = 0.8405\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m13.2428\u001b[0m\n","updated.... average c-index = 0.9160\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m11.7581\u001b[0m\n","updated.... average c-index = 0.9324\n","Current: 0.9324059381178357\n","Current best: 0.9582201292880401\n","Random search... itr: 24\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 300, 'h_dim_CS': 100, 'num_layers_shared': 2, 'num_layers_CS': 5, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 1.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m22.1798\u001b[0m\n","updated.... average c-index = 0.9253\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m18.5857\u001b[0m\n","updated.... average c-index = 0.9296\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m17.0236\u001b[0m\n","Current: 0.929566644413049\n","Current best: 0.9582201292880401\n","Random search... itr: 25\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 300, 'h_dim_CS': 100, 'num_layers_shared': 1, 'num_layers_CS': 1, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 0.1, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m5.3402\u001b[0m\n","updated.... average c-index = 0.9161\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m4.4905\u001b[0m\n","updated.... average c-index = 0.9189\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m4.2900\u001b[0m\n","updated.... average c-index = 0.9216\n","Current: 0.9215597983747922\n","Current best: 0.9582201292880401\n","Random search... itr: 26\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 300, 'h_dim_CS': 200, 'num_layers_shared': 2, 'num_layers_CS': 3, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 3.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m50.4661\u001b[0m\n","updated.... average c-index = 0.9446\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m41.2811\u001b[0m\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m37.3769\u001b[0m\n","Current: 0.9446047906144303\n","Current best: 0.9582201292880401\n","Random search... itr: 27\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 300, 'h_dim_CS': 50, 'num_layers_shared': 2, 'num_layers_CS': 5, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 5.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m100.5629\u001b[0m\n","updated.... average c-index = 0.9321\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m79.5842\u001b[0m\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m71.8170\u001b[0m\n","Current: 0.932065695108772\n","Current best: 0.9582201292880401\n","Random search... itr: 28\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 300, 'h_dim_CS': 50, 'num_layers_shared': 2, 'num_layers_CS': 1, 'active_fn': 'relu', 'alpha': 1.0, 'beta': 1.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m22.1324\u001b[0m\n","updated.... average c-index = 0.9369\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m16.7449\u001b[0m\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m14.8742\u001b[0m\n","Current: 0.9369160856741844\n","Current best: 0.9582201292880401\n","Random search... itr: 29\n","{'mb_size': 128, 'iteration': 3000, 'keep_prob': 0.6, 'lr_train': 0.0001, 'h_dim_shared': 200, 'h_dim_CS': 50, 'num_layers_shared': 1, 'num_layers_CS': 2, 'active_fn': 'tanh', 'alpha': 1.0, 'beta': 5.0, 'gamma': 0}\n","MAIN TRAINING ...\n","EVALUATION TIMES: [24, 48, 72]\n","|| ITR: 1000 | Loss: \u001b[1m\u001b[33m89.4436\u001b[0m\n","updated.... average c-index = 0.9366\n","|| ITR: 2000 | Loss: \u001b[1m\u001b[33m66.9507\u001b[0m\n","updated.... average c-index = 0.9438\n","|| ITR: 3000 | Loss: \u001b[1m\u001b[33m60.7772\u001b[0m\n","Current: 0.9438039846613536\n","Current best: 0.9582201292880401\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qkPQjkXqt9cX"},"source":["### Part 3: Analysis 1"]},{"cell_type":"markdown","metadata":{"id":"k0nkfc6Y5xtj"},"source":["To perform a performance experiment with DeepHit, the respective set of variables used for the experiment needs to be selected first. After that, samples are iteratively loaded. Based on 80% of each sample, a DeepHit model is trained and the performance is recorded by applying the trained model on a test set containing the remaining 20% of each sample. After that, cause-specific means, total means, and means across all sample results are calculated based on the obtained results. To switch between the experiments, one needs to follow the instructions provided in the code on how to do so."]},{"cell_type":"code","metadata":{"id":"G8XVw8V2cots","colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"status":"ok","timestamp":1575478246714,"user_tz":-60,"elapsed":1040598,"user":{"displayName":"Gabriel Blumenstock","photoUrl":"","userId":"15396447649265516687"}},"outputId":"e2c9d376-419a-4248-e762-96caa6ae17e2"},"source":["# ANALYSIS 1 (Experiments 1.1/1.2/1.3/2/3/4.1/4.2/4.3)\n","\n","#for experiments 1.1, 3, and 4.1:\n","cols = [\"int.rate\", \"orig.upb\", \"fico.score\", \"dti.r\",  \"ltv.r\", \"bal.repaid\", \"t.act.12m\", \"t.del.30d.12m\", \"t.del.60d.12m\"]\n","\n","#for experiments 1.2 and 2:\n","#cols = [\"hpi.st.d.t.o\", \"hpi.zip.o\", \"hpi.zip.d.t.o\", \"ppi.c.FRMA\", \"TB10Y.d.t.o\", \"FRMA30Y.d.t.o\", \"ppi.o.FRMA\", \"equity.est\", \"hpi.st.log12m\", \"hpi.r.st.us\", \"hpi.r.zip.st\", \"st.unemp.r12m\", \"st.unemp.r3m\", \"TB10Y.r12m\", \"T10Y3MM\", \"T10Y3MM.r12m\"]\n","\n","#for experiment 1.3:\n","#cols = [\"hpi.st.d.t.o\", \"ppi.c.FRMA\", \"TB10Y.d.t.o\", \"FRMA30Y.d.t.o\", \"ppi.o.FRMA\", \"hpi.r.st.us\", \"st.unemp.r12m\", \"st.unemp.r3m\", \"TB10Y.r12m\", \"T10Y3MM\", \"T10Y3MM.r12m\"]\n","\n","#for experiment 4.2:\n","#cols = [\"hpi.st.d.t.o\", \"ppi.c.FRMA\", \"TB10Y.d.t.o\", \"FRMA30Y.d.t.o\", \"ppi.o.FRMA\", \"hpi.r.st.us\", \"st.unemp.r12m\", \"st.unemp.r3m\", \"TB10Y.r12m\", \"T10Y3MM\", \"T10Y3MM.r12m\"]\n","\n","#for experiment 4.3:\n","#cols = [\"int.rate\", \"orig.upb\", \"fico.score\", \"dti.r\",  \"ltv.r\", \"bal.repaid\", \"t.act.12m\", \"t.del.30d.12m\", \"t.del.60d.12m\", \"hpi.st.d.t.o\", \"ppi.c.FRMA\", \"TB10Y.d.t.o\", \"FRMA30Y.d.t.o\", \"ppi.o.FRMA\", \"hpi.r.st.us\", \"st.unemp.r12m\", \"st.unemp.r3m\", \"TB10Y.r12m\", \"T10Y3MM\", \"T10Y3MM.r12m\"]\n","\n","#experiment 1 -> 2:\n","# activate #!cp and #import_dataset_own_CR\n","# swap \"#\" model predict\n","# swap \"#\" results 1/2\n","\n","#experiment 1 -> 3:\n","# 10x \"loandataX.csv\" -> \"newdataX.csv\"\n","# iterations 3000 -> 1500\n","\n","#experiment 1 -> 4:\n","# 10x \"loandata\" -> \"newdata\"\n","# change hyperparameters\n","\n","ALL_RSLTS = pd.DataFrame(columns=[\"24mth c(1)_index\", \"48mth c(1)_index\", \"72mth c(1)_index\", \"24mth c(2)_index\", \"48mth c(2)_index\", \"72mth c(2)_index\"])\n","\n","for i in range(10):\n","  if i==0:\n","    !cp \"/content/drive/My Drive/datasets/loandata1.csv\" \"loandata.csv\" #\"loandata1s.csv\" for experiment 3 #\"newdata1.csv\" for experiment 4.1/4.2/4.3\n","  elif i==1:\n","    !cp \"/content/drive/My Drive/datasets/loandata2.csv\" \"loandata.csv\" #\"loandata2s.csv\" for experiment 3 #\"newdata2.csv\" for experiment 4.1/4.2/4.3\n","  elif i==2:\n","    !cp \"/content/drive/My Drive/datasets/loandata3.csv\" \"loandata.csv\" #\"loandata3s.csv\" for experiment 3 #\"newdata3.csv\" for experiment 4.1/4.2/4.3\n","  elif i==3:\n","    !cp \"/content/drive/My Drive/datasets/loandata4.csv\" \"loandata.csv\" #\"loandata4s.csv\" for experiment 3 #\"newdata4.csv\" for experiment 4.1/4.2/4.3\n","  elif i==4:\n","    !cp \"/content/drive/My Drive/datasets/loandata5.csv\" \"loandata.csv\" #\"loandata5s.csv\" for experiment 3 #\"newdata5.csv\" for experiment 4.1/4.2/4.3\n","  elif i==5:\n","    !cp \"/content/drive/My Drive/datasets/loandata6.csv\" \"loandata.csv\" #\"loandata6s.csv\" for experiment 3 #\"newdata6.csv\" for experiment 4.1/4.2/4.3\n","  elif i==6:\n","    !cp \"/content/drive/My Drive/datasets/loandata7.csv\" \"loandata.csv\" #\"loandata7s.csv\" for experiment 3 #\"newdata7.csv\" for experiment 4.1/4.2/4.3\n","  elif i==7:\n","    !cp \"/content/drive/My Drive/datasets/loandata8.csv\" \"loandata.csv\" #\"loandata8s.csv\" for experiment 3 #\"newdata8.csv\" for experiment 4.1/4.2/4.3\n","  elif i==8:\n","    !cp \"/content/drive/My Drive/datasets/loandata9.csv\" \"loandata.csv\" #\"loandata9s.csv\" for experiment 3 #\"newdata9.csv\" for experiment 4.1/4.2/4.3\n","  elif i==9:\n","    !cp \"/content/drive/My Drive/datasets/loandata10.csv\" \"loandata.csv\" #\"loandata10s.csv\" for experiment 3 #\"newdata10.csv\" for experiment 4.1/4.2/4.3\n","\n","  (x_dim), (data, time, label), (mask1, mask2) = import_dataset_OWN(norm_mode = 'standard')\n","  EVAL_TIMES = [24, 48, 72]\n","  DATA = (data, time, label)\n","  MASK = (mask1, mask2)\n","\n","  # TRAIN NETWORK BASED ON OPTIMAL HYPERPARAMETERS\n","\n","  in_parser = {'mb_size': 128,\n","                   'iteration': 3000, #1500 for experiment 3 #1000 for experiment 4.2\n","                   'keep_prob': 0.6,\n","                   'lr_train': 1e-4,\n","                   'h_dim_shared': 300,\n","                   'h_dim_CS': 200, #100 for experiment 4.1, 4.2, 4.3\n","                   'num_layers_shared': 3, #1 for experiment 4.1, 4.2, 4.3\n","                   'num_layers_CS':5, #3 for experiment 4.1, 4.2, 4.3\n","                   'active_fn': 'relu',\n","                   'alpha':1.0,\n","                   'beta':5.0, #1.0 for experiment 4.1, 4.2, 4.3\n","                   'gamma':0 }\n","\n","  max_valid = 0.\n","\n","  train_save_deephit(DATA, MASK, in_parser, EVAL_TIMES)\n","\n","  _, num_Event, num_Category  = np.shape(mask1)\n","\n","  mb_size                     = in_parser['mb_size']\n","\n","  iteration                   = in_parser['iteration']\n","\n","  keep_prob                   = in_parser['keep_prob']\n","  lr_train                    = in_parser['lr_train']\n","\n","  h_dim_shared                = in_parser['h_dim_shared']\n","  h_dim_CS                    = in_parser['h_dim_CS']\n","  num_layers_shared           = in_parser['num_layers_shared']\n","  num_layers_CS               = in_parser['num_layers_CS']\n","\n","  if in_parser['active_fn'] == 'relu':\n","      active_fn                = tf.nn.relu\n","  elif in_parser['active_fn'] == 'elu':\n","      active_fn                = tf.nn.elu\n","  elif in_parser['active_fn'] == 'tanh':\n","      active_fn                = tf.nn.tanh\n","  else:\n","      print('Error!')\n","\n","  initial_W                   = tf.contrib.layers.xavier_initializer()\n","\n","  alpha                       = in_parser['alpha']\n","  beta                        = in_parser['beta']\n","  gamma                       = in_parser['gamma']\n","  parameter_name              = 'a' + str('%02.0f' %(10*alpha)) + 'b' + str('%02.0f' %(10*beta)) + 'c' + str('%02.0f' %(10*gamma))\n","\n","  input_dims                  = { 'x_dim'         : x_dim,\n","                                  'num_Event'     : num_Event,\n","                                  'num_Category'  : num_Category}\n","\n","  network_settings            = { 'h_dim_shared'         : h_dim_shared,\n","                                  'h_dim_CS'          : h_dim_CS,\n","                                  'num_layers_shared'    : num_layers_shared,\n","                                  'num_layers_CS'    : num_layers_CS,\n","                                  'active_fn'      : active_fn,\n","                                  'initial_W'         : initial_W }\n","\n","  tf.reset_default_graph()\n","\n","  config = tf.ConfigProto()\n","  config.gpu_options.allow_growth = True\n","  sess = tf.Session(config=config)\n","\n","  model = Model_DeepHit(sess, \"DeepHit\", input_dims, network_settings)\n","  saver = tf.train.Saver()\n","\n","  sess.run(tf.global_variables_initializer())\n","\n","  saver.restore(sess, \"/content/drive/My Drive/deephitmodel/model\")\n","\n","  # EVALUATE MODEL (test data being recreated)\n","\n","  (tr_data,te_data, tr_time,te_time, tr_label,te_label, \n","   tr_mask1,te_mask1, tr_mask2,te_mask2)  = train_test_split(data, time, label, mask1, mask2, test_size=0.2, random_state=seed)\n","\n","  #activate next two lines for experiment_2 (to inject test data from after-crisis dataset)\n","  #!cp \"/content/drive/My Drive/datasets/loandataCR.csv\" \"loandataCR.csv\"\n","  #(x_dimCR), (te_dataCR, te_timeCR, te_labelCR), (te_mask1CR, te_mask2CR) = import_dataset_OWN_CR(norm_mode = 'standard') #loan_cr.csv now from AC dataset\n","\n","  pred = model.predict(te_data)\n","  #activate next line and deactivate last line for experiment_2\n","  #pred = model.predict(te_dataCR)\n","\n","  result = np.zeros([num_Event, len(EVAL_TIMES)])\n","\n","  for t, t_time in enumerate(EVAL_TIMES):\n","      eval_horizon = int(t_time)\n","\n","      if eval_horizon >= num_Category:\n","          print( 'ERROR: evaluation horizon is out of range')\n","          result[:, t] -1\n","      else:\n","          risk = np.sum(pred[:,:,:(eval_horizon+1)], axis=2) #risk score until EVAL_TIMES\n","          for k in range(num_Event):\n","              result[k, t] = c_index(risk[:,k], te_time, (te_label[:,0] == k+1).astype(float), eval_horizon)\n","              #activate the next line and deactivate the previous line for experiment_2\n","              #result[k, t] = c_index(risk[:,k], te_timeCR, (te_labelCR[:,0] == k+1).astype(float), eval_horizon)\n","\n","  df1 = pd.DataFrame(result)\n","\n","  RSLTS = pd.DataFrame(np.zeros((1, 6)))\n","  RSLTS = RSLTS.rename(columns={0: \"24mth c(1)_index\", 1: \"48mth c(1)_index\", 2: \"72mth c(1)_index\", 3: \"24mth c(2)_index\", 4: \"48mth c(2)_index\", 5: \"72mth c(2)_index\"})\n","  RSLTS.iat[0,0] = df1.iat[0,0]\n","  RSLTS.iat[0,1] = df1.iat[0,1]\n","  RSLTS.iat[0,2] = df1.iat[0,2]\n","  RSLTS.iat[0,3] = df1.iat[1,0]\n","  RSLTS.iat[0,4] = df1.iat[1,1]\n","  RSLTS.iat[0,5] = df1.iat[1,2]\n","\n","  ALL_RSLTS = pd.concat([ALL_RSLTS, RSLTS])\n","\n","ALL_RSLTS[\"c_mean\"] = ALL_RSLTS.mean(axis=1)\n","tmp1 = ALL_RSLTS.iloc[:,0:3].copy()\n","tmp1[\"c(1)_mean\"] = tmp1.mean(axis=1)\n","tmp2 = ALL_RSLTS.iloc[:,3:6].copy()\n","tmp2[\"c(2)_mean\"] = tmp2.mean(axis=1)\n","ALL_RSLTS = pd.concat([ALL_RSLTS.iloc[:,0:6], tmp1.iloc[:,3], tmp2.iloc[:,3], ALL_RSLTS.iloc[:,6]], axis=1)\n","ALL_RSLTS = ALL_RSLTS.append(ALL_RSLTS.mean(axis=0), ignore_index=True)\n","ALL_RSLTS = ALL_RSLTS.rename(index={0: \"1\", 1: \"2\", 2: \"3\", 3: \"4\", 4: \"5\", 5: \"6\", 6: \"7\", 7: \"8\", 8: \"9\", 9: \"10\", 10: \"col_mean\"})\n","ALL_RSLTS = ALL_RSLTS.multiply(100)\n","ALL_RSLTS = ALL_RSLTS.round(2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/drive/My Drive/deephitmodel/model\n","INFO:tensorflow:Restoring parameters from /content/drive/My Drive/deephitmodel/model\n","INFO:tensorflow:Restoring parameters from /content/drive/My Drive/deephitmodel/model\n","INFO:tensorflow:Restoring parameters from /content/drive/My Drive/deephitmodel/model\n","INFO:tensorflow:Restoring parameters from /content/drive/My Drive/deephitmodel/model\n","INFO:tensorflow:Restoring parameters from /content/drive/My Drive/deephitmodel/model\n","INFO:tensorflow:Restoring parameters from /content/drive/My Drive/deephitmodel/model\n","INFO:tensorflow:Restoring parameters from /content/drive/My Drive/deephitmodel/model\n","INFO:tensorflow:Restoring parameters from /content/drive/My Drive/deephitmodel/model\n","INFO:tensorflow:Restoring parameters from /content/drive/My Drive/deephitmodel/model\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Iw6OuZ2aBCFl"},"source":["### DOWNLOAD RESULTS\n","ALL_RSLTS.to_csv(\"experiment_1_1_DHT.csv\")\n","files.download(\"experiment_1_1_DHT.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Wbo5TBeuKvW"},"source":["### Part 4: Analysis 2"]},{"cell_type":"markdown","metadata":{"id":"2t1i9PsC50Of"},"source":["To perform the experiments of Analysis 2, a DeepHit model is first trained based on 80% of the \"loandataVI.csv\"/\"newdataVI.csv\" dataset, and performance is evaluated based on the remaining 20%. After that, each variable is subsequently noised-up in the test set while restoring the previously noised-up variables back to their original values, and performance is measured again. Based on the performance differences, variable importance rankings are calculated. The entire procedure can then be repeated for different standard deviations. Thereby, the standard deviation is set by the \"ND_STD\" variable found in the beginning of the cell. As we do not need to train the DeepHit model again when changing the standard deviation of the noise term, we can prevent repeated training of the model by setting the variable \"tyn\", also found at the beginning of the cell, equal to \"0\".\n","\n","To perform experiments A to F, one needs to select the respective variables, the respective dataset, and the respective parser, as documented in the code."]},{"cell_type":"code","metadata":{"id":"8ZjjzSAXA0ls","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1575478405429,"user_tz":-60,"elapsed":116088,"user":{"displayName":"Gabriel Blumenstock","photoUrl":"","userId":"15396447649265516687"}},"outputId":"2fe55809-e567-4b75-a3c5-1e33f4a37187"},"source":["# ANALYSIS 2 (Experiments A/B/C/D/E/F)\n","\n","tyn = 1 #train (1) for the first variance setting, after that do not train (0)\n","ND_STD = 10 #set the standard deviation of the random normal noise\n","\n","#for experiments A and D:\n","cols = [\"int.rate\", \"orig.upb\", \"fico.score\", \"dti.r\",  \"ltv.r\", \"bal.repaid\", \"t.act.12m\", \"t.del.30d.12m\", \"t.del.60d.12m\"]\n","\n","#for experiment B:\n","#cols = [\"hpi.st.d.t.o\", \"hpi.zip.o\", \"hpi.zip.d.t.o\", \"ppi.c.FRMA\", \"TB10Y.d.t.o\", \"FRMA30Y.d.t.o\", \"ppi.o.FRMA\", \"equity.est\", \"hpi.st.log12m\", \"hpi.r.st.us\", \"hpi.r.zip.st\", \"st.unemp.r12m\", \"st.unemp.r3m\", \"TB10Y.r12m\", \"T10Y3MM\", \"T10Y3MM.r12m\"]\n","\n","#for experiment C:\n","#cols = [\"int.rate\", \"orig.upb\", \"fico.score\", \"dti.r\",  \"ltv.r\", \"bal.repaid\", \"t.act.12m\", \"t.del.30d.12m\", \"t.del.60d.12m\", \"hpi.st.d.t.o\", \"hpi.zip.o\", \"hpi.zip.d.t.o\", \"ppi.c.FRMA\", \"TB10Y.d.t.o\", \"FRMA30Y.d.t.o\", \"ppi.o.FRMA\", \"equity.est\", \"hpi.st.log12m\", \"hpi.r.st.us\", \"hpi.r.zip.st\", \"st.unemp.r12m\", \"st.unemp.r3m\", \"TB10Y.r12m\", \"T10Y3MM\", \"T10Y3MM.r12m\"]\n","\n","#for experiment E and reimplementation of experiment B:\n","#cols = [\"hpi.st.d.t.o\", \"ppi.c.FRMA\", \"TB10Y.d.t.o\", \"FRMA30Y.d.t.o\", \"ppi.o.FRMA\", \"hpi.r.st.us\", \"st.unemp.r12m\", \"st.unemp.r3m\", \"TB10Y.r12m\", \"T10Y3MM\", \"T10Y3MM.r12m\"]\n","\n","#for experiment F and reimplementation of experiment C:\n","#cols = [\"int.rate\", \"orig.upb\", \"fico.score\", \"dti.r\",  \"ltv.r\", \"bal.repaid\", \"t.act.12m\", \"t.del.30d.12m\", \"t.del.60d.12m\", \"hpi.st.d.t.o\", \"ppi.c.FRMA\", \"TB10Y.d.t.o\", \"FRMA30Y.d.t.o\", \"ppi.o.FRMA\", \"hpi.r.st.us\", \"st.unemp.r12m\", \"st.unemp.r3m\", \"TB10Y.r12m\", \"T10Y3MM\", \"T10Y3MM.r12m\"]\n","\n","#for experiments A, B, C:\n","!cp \"/content/drive/My Drive/datasets/loandataVI.csv\" \"loandata.csv\"\n","\n","#for experiments D, E, F:\n","#!cp \"/content/drive/My Drive/datasets/newdataVI.csv\" \"loandata.csv\"\n","\n","(x_dim), (data, time, label), (mask1, mask2) = import_dataset_OWN(norm_mode = 'standard')\n","EVAL_TIMES = [24, 48, 72]\n","DATA = (data, time, label)\n","MASK = (mask1, mask2)\n","\n","ALL_RSLTS = pd.DataFrame(columns=[\"24mth c(1)_index\", \"48mth c(1)_index\", \"72mth c(1)_index\", \"24mth c(2)_index\", \"48mth c(2)_index\", \"72mth c(2)_index\"])\n","\n","for i in range(x_dim+1):\n","  if i==0:\n","    if tyn==1:\n","\n","    # TRAIN MODEL BASED ON OPTIMAL HYPERPARAMETERS\n","\n","     #for experiments A, B, C:\n","      in_parser = {'mb_size': 128,\n","                       'iteration': 3000,\n","                       'keep_prob': 0.6,\n","                       'lr_train': 1e-4,\n","                       'h_dim_shared': 300,\n","                       'h_dim_CS': 200,\n","                       'num_layers_shared': 3,\n","                       'num_layers_CS':5,\n","                       'active_fn': 'relu',\n","                       'alpha':1.0,\n","                       'beta':5.0,\n","                       'gamma':0 }\n","\n","      #for experiments D, E, F:\n","      #in_parser = {'mb_size': 128,\n","      #                 'iteration': 3000, #1000 for experiment E\n","      #                 'keep_prob': 0.6,\n","      #                 'lr_train': 1e-4,\n","      #                 'h_dim_shared': 300,\n","      #                 'h_dim_CS': 100,\n","      #                 'num_layers_shared': 1,\n","      #                 'num_layers_CS':3,\n","      #                 'active_fn': 'relu',\n","      #                 'alpha':1.0,\n","      #                 'beta':1.0,\n","      #                 'gamma':0 }\n","\n","      max_valid = 0.\n","\n","      train_save_deephit(DATA, MASK, in_parser, EVAL_TIMES)\n","\n","      _, num_Event, num_Category  = np.shape(mask1)\n","\n","      mb_size                     = in_parser['mb_size']\n","\n","      iteration                   = in_parser['iteration']\n","\n","      keep_prob                   = in_parser['keep_prob']\n","      lr_train                    = in_parser['lr_train']\n","\n","      h_dim_shared                = in_parser['h_dim_shared']\n","      h_dim_CS                    = in_parser['h_dim_CS']\n","      num_layers_shared           = in_parser['num_layers_shared']\n","      num_layers_CS               = in_parser['num_layers_CS']\n","\n","      if in_parser['active_fn'] == 'relu':\n","          active_fn                = tf.nn.relu\n","      elif in_parser['active_fn'] == 'elu':\n","          active_fn                = tf.nn.elu\n","      elif in_parser['active_fn'] == 'tanh':\n","          active_fn                = tf.nn.tanh\n","      else:\n","          print('Error!')\n","\n","      initial_W                   = tf.contrib.layers.xavier_initializer()\n","\n","      alpha                       = in_parser['alpha']\n","      beta                        = in_parser['beta']\n","      gamma                       = in_parser['gamma'] \n","      parameter_name              = 'a' + str('%02.0f' %(10*alpha)) + 'b' + str('%02.0f' %(10*beta)) + 'c' + str('%02.0f' %(10*gamma))\n","\n","      input_dims                  = { 'x_dim'         : x_dim,\n","                                    'num_Event'     : num_Event,\n","                                    'num_Category'  : num_Category}\n","\n","      network_settings            = { 'h_dim_shared'         : h_dim_shared,\n","                                      'h_dim_CS'          : h_dim_CS,\n","                                      'num_layers_shared'    : num_layers_shared,\n","                                      'num_layers_CS'    : num_layers_CS,\n","                                      'active_fn'      : active_fn,\n","                                      'initial_W'         : initial_W }\n","\n","\n","    tf.reset_default_graph()\n","\n","    config = tf.ConfigProto()\n","    config.gpu_options.allow_growth = True\n","    sess = tf.Session(config=config)\n","\n","    model = Model_DeepHit(sess, \"DeepHit\", input_dims, network_settings)\n","    saver = tf.train.Saver()\n","\n","    sess.run(tf.global_variables_initializer())\n","\n","    saver.restore(sess, \"/content/drive/My Drive/deephitmodel/model\")\n","\n","  # EVALUATE MODEL (test data being recreated)\n","\n","    (tr_data,te_data, tr_time,te_time, tr_label,te_label, \n","     tr_mask1,te_mask1, tr_mask2,te_mask2)  = train_test_split(data, time, label, mask1, mask2, test_size=0.2, random_state=seed)  #for experiment 3_1: test_size=0.01336362\n","\n","  if i!=0:\n","    (tr_data,te_data, tr_time,te_time, tr_label,te_label, \n","     tr_mask1,te_mask1, tr_mask2,te_mask2)  = train_test_split(data, time, label, mask1, mask2, test_size=0.2, random_state=seed)  #for experiment 3_1: test_size=0.01336362\n","\n","    noise_col = i-1\n","    te_data[:,noise_col] = te_data[:,noise_col]+np.random.normal(0, ND_STD, te_data.shape[0])\n","\n","  pred = model.predict(te_data)\n","\n","  result = np.zeros([num_Event, len(EVAL_TIMES)])\n","\n","  for t, t_time in enumerate(EVAL_TIMES):\n","      eval_horizon = int(t_time)\n","\n","      if eval_horizon >= num_Category:\n","          print( 'ERROR: evaluation horizon is out of range')\n","          result[:, t] -1\n","      else:\n","          risk = np.sum(pred[:,:,:(eval_horizon+1)], axis=2) #risk score until EVAL_TIMES\n","          for k in range(num_Event):\n","              result[k, t] = c_index(risk[:,k], te_time, (te_label[:,0] == k+1).astype(float), eval_horizon)\n","              #activate the next line and deactivate the previous line for experiment_2\n","              #result[k, t] = c_index(risk[:,k], te_timeCR, (te_labelCR[:,0] == k+1).astype(float), eval_horizon)\n","\n","  df1 = pd.DataFrame(result)\n","\n","  RSLTS = pd.DataFrame(np.zeros((1, 6)))\n","  RSLTS = RSLTS.rename(columns={0: \"24mth c(1)_index\", 1: \"48mth c(1)_index\", 2: \"72mth c(1)_index\", 3: \"24mth c(2)_index\", 4: \"48mth c(2)_index\", 5: \"72mth c(2)_index\"})\n","  RSLTS.iat[0,0] = df1.iat[0,0]\n","  RSLTS.iat[0,1] = df1.iat[0,1]\n","  RSLTS.iat[0,2] = df1.iat[0,2]\n","  RSLTS.iat[0,3] = df1.iat[1,0]\n","  RSLTS.iat[0,4] = df1.iat[1,1]\n","  RSLTS.iat[0,5] = df1.iat[1,2]\n","\n","  ALL_RSLTS = pd.concat([ALL_RSLTS, RSLTS])\n","\n","ALL_RSLTS[\"c_mean\"] = ALL_RSLTS.mean(axis=1)\n","ALL_RSLTS = ALL_RSLTS.reset_index()\n","ALL_RSLTS = ALL_RSLTS.drop(ALL_RSLTS.columns[0], axis=1)\n","ALL_RSLTS = ALL_RSLTS.rename(index={0: \"No Noise\"})\n","for x in range(x_dim):\n","  ALL_RSLTS = ALL_RSLTS.rename(index={x+1: cols[x]})\n","tmp1 = ALL_RSLTS.iloc[:,0:3].copy()\n","tmp1[\"c(1)_mean\"] = tmp1.mean(axis=1)\n","tmp2 = ALL_RSLTS.iloc[:,3:6].copy()\n","tmp2[\"c(2)_mean\"] = tmp2.mean(axis=1)\n","ALL_RSLTS = pd.concat([ALL_RSLTS.iloc[:,0:6], tmp1.iloc[:,3], tmp2.iloc[:,3], ALL_RSLTS.iloc[:,6]], axis=1)\n","ALL_RSLTS = ALL_RSLTS.multiply(100)\n","ALL_RSLTSX = ALL_RSLTS.copy()\n","ALL_RSLTSX = ALL_RSLTSX.diff().cumsum()\n","ALL_RSLTSXX = ALL_RSLTSX.copy()\n","ALL_RSLTSXX['24mth c(1)_index'] = ALL_RSLTSXX['24mth c(1)_index'].rank(method='min', na_option='keep')\n","ALL_RSLTSXX['48mth c(1)_index'] = ALL_RSLTSXX['48mth c(1)_index'].rank(method='min', na_option='keep')\n","ALL_RSLTSXX['72mth c(1)_index'] = ALL_RSLTSXX['72mth c(1)_index'].rank(method='min', na_option='keep')\n","ALL_RSLTSXX['24mth c(2)_index'] = ALL_RSLTSXX['24mth c(2)_index'].rank(method='min', na_option='keep')\n","ALL_RSLTSXX['48mth c(2)_index'] = ALL_RSLTSXX['48mth c(2)_index'].rank(method='min', na_option='keep')\n","ALL_RSLTSXX['72mth c(2)_index'] = ALL_RSLTSXX['72mth c(2)_index'].rank(method='min', na_option='keep')\n","ALL_RSLTSXX['c(1)_mean'] = ALL_RSLTSXX['c(1)_mean'].rank(method='min', na_option='keep')\n","ALL_RSLTSXX['c(2)_mean'] = ALL_RSLTSXX['c(2)_mean'].rank(method='min', na_option='keep')\n","ALL_RSLTSXX['c_mean'] = ALL_RSLTSXX['c_mean'].rank(method='min', na_option='keep')\n","ALL_RSLTS_ALL = pd.concat([ALL_RSLTS,ALL_RSLTSX, ALL_RSLTSXX])\n","ALL_RSLTS_ALL = ALL_RSLTS_ALL.round(2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /content/drive/My Drive/deephitmodel/model\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FMLgHUnLFfP1"},"source":["### DOWNLOAD RESULTS\n","ALL_RSLTS_ALL.to_csv(\"experiment_A_10_DHT.csv\")\n","files.download(\"experiment_A_10_DHT.csv\")"],"execution_count":null,"outputs":[]}]}